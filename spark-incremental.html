<!DOCTYPE html>
<html>
  <head>
    <title>Incremental updates for ML models</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Exact and approximate incremental updates for some machine learning models
## Robert Dodier

---
# Problem statement
--

 * We're building ML models from lots of data
--

 * We get more data from time to time
--

 * We want to update the model with the new data
--

 * In general, it's necessary to start over with the whole data set
--

 * Under what circumstances can we work with just the new data?

---
# Road map

 * First let's look at a general theoretical answer
 * Then we'll consider a simple model for which the theoretical answer is directly applicable
 * ... and look at some Scala code for that
 * Finally we'll expand our consideration to more complex models for which the theory is not directly applicable
---
# Short answer
--

 * Exact incremental update == model has "sufficient statistics"
--

 * "Sufficient" means: if you have these statistics, you can throw away anything else
--

 * Not all models have sufficient statistics
--

 * ... but some interesting models do: linear regression, logistic regression, quadratic discriminant

---
# Quadratic discriminant classifier

--

 * Given `\(p(x|c)\)` is a Gaussian density for each class `\(c\)`

--

 * then the surfaces of constant `\(p(c|x)\)` are conic sections

--

 * When both classes have the same covariance, the surfaces of constant `\(p(c|x)\)` are planes

---
## Scatterplots of samples from `\(p(x|c)\)`

![samples scatterplot](qd_points.svg)

---
## Surfaces of constant `\(p(c|x)\)` w/ general covariance

![p(c|x) contours](qd_contours.svg)

---
## Surfaces of constant `\(p(c|x)\)` w/ equal covariance

![pooled-covariance p(c|x) contours](qd_pooled_contours.svg)

---
# Quadratic discriminant and sufficient statistics

--

 * s.s. for q.d. are the count, sum, and sum of squares for each class
 * ... from those we can compute model parameters
 
---
# Scala code for q.d.

```scala
```

---
# Longer answer

--

 * More speculatively, some models which have latent / hidden / missing variables
--

   * often trained with expectation-maximization algorithms: E step = estimate missing variables,
    M step = maximum likelihood given missing
--

   * M step may have sufficient statistics
--

   * ... so the model "almost" or "sort of" has sufficient statistics
--

   * e.g. Gaussian mixture, topic-word models
--

   * maybe neural networks and tree-structured models as well?
--

     * for n.n., hidden variables might be the parameters for the hidden units
--

     * for tree models, hidden variables might be the location of splits in different dimensions
--

 * Can we exploit s.s., for example, by fixing the missing variables and carrying out only the M step?

---
# Cross-validation and sufficient statistics
--

 * CV = train on most of data, test on hold-out data; rotate blocks and repeat
--

 * Sufficient statistics can help us calculate CV!
--

 * Compute s.s. for each block and add them all up
--

 * For each block, subtract s.s. from total and use what's left to compute model parameters
--

 * ... and then test the model on the held-out block

---
# Conclusions and directions for further investigation
--

 * Working with s.s. yields exactly the same result as working with all data
 * ... so that can greatly speed up some common ML operations
--

 * But s.s. exist only for some relatively simple models
 * ... and for models in wide use (trees, neural networks), s.s. depend on hidden variables
--

 * For trees, hidden variables = splitting planes + leaf counts
 * ... we can approximate that pretty closely, so we can have an almost-exact incremental update
--

 * For neural networks, hidden variables = parameters for hidden units
 * ... I don't see any easy way to approximate those parameters
 * ... but maybe s.s. could figure in a fast scheme for searching over random hidden units
--

 * Low-hanging fruit:
 * ... incremental update for Spark random forest
 * ... including exploiting s.s. in CV
--

 * Exploiting s.s. for neural network training seems a little farther out,
   and maybe less certain to yield something useful

# Just for fun

 * Let's play "Spot the Monad"
 * Looks like the s.s. update is sort of like a state monad
 * ... add some stuff to s.s., output result and update s.s.
 * Working out the details with lots of help from tpolecat
    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
