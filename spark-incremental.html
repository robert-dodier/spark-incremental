<!DOCTYPE html>
<html>
  <head>
    <title>Incremental updates for ML models</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Exact and approximate incremental updates for some machine learning models
## Robert Dodier

---
# Problem statement
--

 * We're building ML models from lots of data
--

 * We get more data from time to time
--

 * We want to update the model with the new data
--

 * In general, it's necessary to start over with the whole data set
--

 * Under what circumstances can we work with just the new data?

---
# Road map

 * First let's look at a general theoretical answer
 * Then we'll consider a simple model for which the theoretical answer is directly applicable
 * ... and look at some Scala code for that
 * Finally we'll expand our consideration to more complex models for which the theory is not directly applicable
---
# Short answer
--

 * Exact incremental update == model has "sufficient statistics"
--

 * "Sufficient" means: if you have these statistics, you can throw away anything else
--

 * Not all models have sufficient statistics
--

 * ... but some interesting models do: linear regression, logistic regression, quadratic discriminant

---
# Quadratic discriminant classifier

--

 * Given `\(p(x|c)\)` is a Gaussian density for each class `\(c\)`

--

 * then the surfaces of constant `\(p(c|x)\)` are conic sections

--

 * When both classes have the same covariance, the surfaces of constant `\(p(c|x)\)` are planes

---
## Scatterplots of samples from `\(p(x|c)\)`

![samples scatterplot](qd_points.svg)

---
## Surfaces of constant `\(p(c|x)\)` w/ general covariance

![p(c|x) contours](qd_contours.svg)

---
## Surfaces of constant `\(p(c|x)\)` w/ equal covariance

![pooled-covariance p(c|x) contours](qd_pooled_contours.svg)

---
# Quadratic discriminant and sufficient statistics

--

 * s.s. for q.d. are the count, sum, and sum of squares for each class
 * ... from those we can compute model parameters
 
---
# Scala code for quadratic discriminant

All the code, data, scripts, and documents are on Github:
[robert-dodier/spark-incremental](https://github.com/robert-dodier/spark-incremental)

Here's something to keep sufficient statistics.

```scala
case class QuadraticDiscriminantSufficientStatistics (
  N : Long,
  N1 : Long,
  N0 : Long,
  X1_sum : breeze.linalg.DenseVector [Double],
  X0_sum : breeze.linalg.DenseVector [Double],
  X1_sum2 : breeze.linalg.DenseMatrix [Double],
  X0_sum2 : breeze.linalg.DenseMatrix [Double]) {

  def add (s: QuadraticDiscriminantSufficientStatistics) = { ... }

  def subtract (s: QuadraticDiscriminantSufficientStatistics) = { ... }
}
```

---
Compute sufficient statistics for new data. 

```scala
    val X1 = X.filter {case LabeledPoint (l, f) => l == 1.0} ...
    val X0 = X.filter {case LabeledPoint (l, f) => l != 1.0} ...

    val N1 = X1.count
    val N0 = X0.count
    val N = N1 + N0

    val m = X.take (1)(0).features.size
    val z = breeze.linalg.DenseVector.zeros[Double] (m)
    val X1_sum = X1.aggregate (z) (_ + _, _ + _)
    val X0_sum = X0.aggregate (z) (_ + _, _ + _)

    val zz = breeze.linalg.DenseMatrix.zeros[Double] (m, m)
    val X1_sum2 = X1.aggregate (zz) ((S, v) => S + v * v.t, (S, T) => S + T)
    val X0_sum2 = X0.aggregate (zz) ((S, v) => S + v * v.t, (S, T) => S + T)
```

The `\((i, j)\)` element of the correlation matrices is defined as `\(\sum_k x_{ki} x_{kj}\)`,
i.e. an inner product on columns `\(i\)` and `\(j\)`.

Note that the outer product of row `\(k\)` with itself is a matrix with `\((i, j)\)` element `\(x_{ki} x_{kj}\)`.
So just add up all those products to get the correlation matrix.

This is perhaps the most concise way to phrase the computation, given that RDD's favor row operations.

---
Compute parameters for the quadratic discriminant model from sufficient statistics.

```scala
  val mean1 = (1.0/summary.N1) * summary.X1_sum
  val mean0 = (1.0/summary.N0) * summary.X0_sum

  val cov1 = (1.0/summary.N1) * summary.X1_sum2 - mean1 * mean1.t
  val cov0 = (1.0/summary.N0) * summary.X0_sum2 - mean0 * mean0.t

  val cov1_det = det (cov1)
  val cov0_det = det (cov0)

  val cov1_inv = inv (cov1)
  val cov0_inv = inv (cov0)

  val pc1 = summary.N1 / summary.N.toDouble
  val pc0 = summary.N0 / summary.N.toDouble
```

The parameters are the mean, covariance, and prior class probability for each of two classes (1 and 0).

Cache the determinant and inverse of the covariance since those are needed to score a new input.

---
Scoring function. This is the posterior log-odds `\(\log(p(c=1|x)/p(c=0|x))\)` for a new input `\(x\)`.

```scala
  def score (x: org.apache.spark.mllib.linalg.Vector): Double = {

    val v = breeze.linalg.DenseVector (x.toArray)
    val m = v.size

    val u1 = v - mean1
    val qf1 = u1.t * (cov1_inv * u1);
    val log_pxc1 = -1/2.0 * qf1 - m/2.0 * Math.log (2*Math.PI)
                     - 1/2.0 * Math.log (cov1_det)

    val u0 = v - mean0
    val qf0 = u0.t * (cov0_inv * u0);
    val log_pxc0 = -1/2.0 * qf0 - m/2.0 * Math.log (2*Math.PI)
                     - 1/2.0 * Math.log (cov0_det)

    val log_pc1 = Math.log (pc1)
    val log_pc0 = Math.log (pc0)

    (log_pxc1 - log_pxc0) + (log_pc1 - log_pc0)
  }
```

For each class `\(c'\)`, `\(p(c=c'|x) = p(x|c=c') p(c')/p(x)\)`
with `\(p(x|c=c') = (2\pi)^{-m/2} |\Sigma_{c'}|^{-1/2} \exp(-\frac{1}{2} (x - \mu_{c'})^T \Sigma_{c'}^{-1} (x - \mu_{c'}))\)`.

At this point we have all that's needed for a basic implementation.

---
# A test program

```scala
    val X_all = MLUtils.loadLibSVMFile (sc, "xyc.data-all-libsvm")
    val qd_all = new QuadraticDiscriminant (X_all)

    val X_99pct = MLUtils.loadLibSVMFile (sc, "xyc.data-99%-libsvm")
    val qd_99pct = new QuadraticDiscriminant (X_99pct)

    val X_1pct = MLUtils.loadLibSVMFile (sc, "xyc.data-1%-libsvm")
    val qd_1pct_only = new QuadraticDiscriminant (X_1pct)

    val qd_1pct_plus_99pct = new QuadraticDiscriminant (qd_99pct.summary, X_1pct)
```
---
# Output w/ only 1% of data

![posterior log-odds, 1% data only](qd_1pct_only.svg)

---
# Output w/ 1% + summary of 99%

![posterior log-odds, 1% data + 99% s.s.](qd_1pct_plus_99pct.svg)

---
# Output w/ all data

![posterior log-odds, all data](qd_all.svg)

---
# Cross-validation and sufficient statistics
--

 * CV = train on most of data, test on hold-out data; rotate blocks and repeat
--

 * Sufficient statistics can help us calculate CV!
--

 * Compute s.s. for each block and add them all up
--

 * For each block, subtract s.s. from total and use what's left to compute model parameters
--

 * ... and then test the model on the held-out block

---
Implementation of cross validation, making use of sufficient statistics.

```scala
  def crossValidation (X: RDD [LabeledPoint], n: Integer): Double = {
    val X_split = X.randomSplit (Array.fill[Double](n)(1.0))
    val summaries = X_split.map (rdd => sufficientStatistics (rdd))
    val all_summary = summaries.reduce ((a, b) => a.add (b))

    val empty = X.context.emptyRDD [LabeledPoint]
    X_split.zip (summaries)
      .map { case (rdd, s) => (new QuadraticDiscriminant
                                     (all_summary.subtract (s), empty))
                                .crossEntropy (rdd) }
      .sum
  }
```

which uses cross entropy as the goodness of fit:

```scala
  def crossEntropy (x: LabeledPoint): Double = {
    val log_odds = score (x.features)
    val log_pc1x = - Math.log (1 + Math.exp (- log_odds))
    val log_pc0x = - Math.log (1 + Math.exp (+ log_odds))
    - x.label * log_pc1x - (1 - x.label) * log_pc0x
  }

  def crossEntropy (X: RDD[LabeledPoint]): Double = {
    X.map (x => crossEntropy (x)).sum
  }
```
---
# Longer answer

--

 * More speculatively, some models which have latent / hidden / missing variables
--

   * often trained with expectation-maximization algorithms: E step = estimate missing variables,
    M step = maximum likelihood given missing
--

   * M step may have sufficient statistics
--

   * ... so the model "almost" or "sort of" has sufficient statistics
--

   * e.g. Gaussian mixture, topic-word models
--

   * maybe neural networks and tree-structured models as well?
--

     * for n.n., hidden variables might be the parameters for the hidden units
--

     * for tree models, hidden variables might be the location of splits in different dimensions
--

 * Can we exploit s.s., for example, by fixing the missing variables and carrying out only the M step?

---
# Conclusions and directions for further investigation
--

 * Working with sufficient statistics yields exactly the same result as working with all data
 * ... so that can greatly speed up some common ML operations
--

 * But s.s. exist only for some relatively simple models
 * ... and for models in wide use (trees, neural networks), s.s. depend on hidden variables
--

 * For trees, hidden variables = splitting planes + leaf counts
 * ... we can approximate that pretty closely, so we can have an almost-exact incremental update
--

 * For neural networks, hidden variables = parameters for hidden units
 * ... I don't see any easy way to approximate those parameters
 * ... but maybe s.s. could figure in a fast scheme for searching over random hidden units
--

 * Low-hanging fruit:
 * ... incremental update for Spark random forest
 * ... including exploiting s.s. in CV
--

 * Exploiting s.s. for neural network training seems a little farther out,
   and maybe less certain to yield something useful

---
# Just for fun

 * Let's play "Spot the Monad"
 * Looks like the sufficient statistics update is sort of like a state monad
 * ... add some stuff to s.s., output result and update s.s.
 * Working out the details with lots of help from tpolecat
    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
