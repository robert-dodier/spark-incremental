Exact incremental updates for some machine learning models
Robert Dodier

Problem statement
 * We're building ML models from lots of data
 * We get more data from time to time
 * We want to update the model with the new data
 * In general, it's necessary to start over with the whole data set
 * Under what circumstances can we work with just the new data?

Short answer
 * Exact incremental update == model has ``sufficient statistics''
 * ``Sufficient'' means: if you have these statistics, you can throw away anything else
 * Not all models have sufficient statistics
 * ... but some interesting models do: linear regression, logistic regression, quadratic discriminant
 * More speculatively, some models which have latent / hidden / missing variables
 ** often trained with expectation-maximization algorithms -- E step = estimate missing variables,
    M step = maximum likelihood given missing
 ** M step may have sufficient statistics
 ** ... so the model ``almost'' or ``sort of'' has sufficient statistics
 ** e.g. Gaussian mixture, topic-word models
 ** maybe neural networks and tree-structured models as well?
 *** for n.n., hidden variables might be the parameters for the hidden units
 *** for tree models, hidden variables might be the location of splits in different dimensions
 * Can we exploit s.s., for example, by fixing the missing variables and carrying out only the M step?

Cross-validation and sufficient statistics
 * CV = train on most of data, test on hold-out data; rotate blocks and repeat
 * Sufficient statistics can help us calculate CV!
 * Compute SS for each block and add them all up
 * For each block, subtract SS from total and use what's left to compute model parameters
 * ... and then test the model on the held-out block

Conclusions and directions for further investigation
 * Working with SS yields exactly the same result as working with all data
 * ... so that can greatly speed up some common ML operations
 * But SS exist only for some relatively simple models
 * ... and for models in wide use (trees, neural networks), SS depend on hidden variables
 * For trees, hidden variables = splitting planes + leaf counts
 * ... we can approximate that pretty closely, so we can have an almost-exact incremental update
 * For neural networks, hidden variables = parameters for hidden units
 * ... I don't see any easy way to approximate those parameters
 * ... but maybe SS could figure in a fast scheme for searching over random hidden units
 * Low-hanging fruit:
 * ... incremental update for Spark random forest
 * ... including exploiting SS in CV
